{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-ollama 0.3.2 requires langchain-core<1.0.0,>=0.3.52, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-mistralai 0.2.9 requires langchain-core<1.0.0,>=0.3.47, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-text-splitters 0.2.2 requires langchain-core<0.3.0,>=0.2.10, but you have langchain-core 0.1.23 which is incompatible.\n",
      "langchain-ibm 0.3.10 requires langchain-core<0.4.0,>=0.3.39, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.1.0 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87\n",
      "Successfully installed langchain-0.1.0 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-ibm==0.0.8 (from versions: 0.0.1, 0.1.0, 0.1.1, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9rc0, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.2.0, 0.2.1, 0.2.2, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.5, 0.3.6, 0.3.7, 0.3.8, 0.3.9, 0.3.10)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-ibm==0.0.8\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-ibm==0.0.8 (from versions: 0.0.1, 0.1.0, 0.1.1, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9rc0, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.2.0, 0.2.1, 0.2.2, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.5, 0.3.6, 0.3.7, 0.3.8, 0.3.9, 0.3.10)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-ibm==0.0.8\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Ignored the following yanked versions: 0.0.9, 0.2.14\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-community==0.1.0 (from versions: 0.0.1rc1, 0.0.1rc2, 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.0.20, 0.0.21, 0.0.22, 0.0.23, 0.0.24, 0.0.25, 0.0.26, 0.0.27, 0.0.28, 0.0.29, 0.0.30, 0.0.31, 0.0.32, 0.0.33, 0.0.34, 0.0.35, 0.0.36, 0.0.37, 0.0.38, 0.2.0rc1, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.2.9, 0.2.10, 0.2.11, 0.2.12, 0.2.13, 0.2.15, 0.2.16, 0.2.17, 0.2.18, 0.2.19, 0.3.0.dev1, 0.3.0.dev2, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.5, 0.3.6, 0.3.7, 0.3.8, 0.3.9, 0.3.10, 0.3.11, 0.3.12, 0.3.13, 0.3.14, 0.3.15, 0.3.16, 0.3.17rc1, 0.3.17, 0.3.18, 0.3.19, 0.3.20, 0.3.21)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-community==0.1.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Ignored the following yanked versions: 0.0.9, 0.2.14\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-community==0.1.0 (from versions: 0.0.1rc1, 0.0.1rc2, 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.0.20, 0.0.21, 0.0.22, 0.0.23, 0.0.24, 0.0.25, 0.0.26, 0.0.27, 0.0.28, 0.0.29, 0.0.30, 0.0.31, 0.0.32, 0.0.33, 0.0.34, 0.0.35, 0.0.36, 0.0.37, 0.0.38, 0.2.0rc1, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.2.7, 0.2.9, 0.2.10, 0.2.11, 0.2.12, 0.2.13, 0.2.15, 0.2.16, 0.2.17, 0.2.18, 0.2.19, 0.3.0.dev1, 0.3.0.dev2, 0.3.0, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.5, 0.3.6, 0.3.7, 0.3.8, 0.3.9, 0.3.10, 0.3.11, 0.3.12, 0.3.13, 0.3.14, 0.3.15, 0.3.16, 0.3.17rc1, 0.3.17, 0.3.18, 0.3.19, 0.3.20, 0.3.21)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-community==0.1.0\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-ollama 0.3.2 requires langchain-core<1.0.0,>=0.3.52, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain 0.1.0 requires langchain-core<0.2,>=0.1.7, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain-mistralai 0.2.9 requires langchain-core<1.0.0,>=0.3.47, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain-text-splitters 0.2.2 requires langchain-core<0.3.0,>=0.2.10, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain-ibm 0.3.10 requires langchain-core<0.4.0,>=0.3.39, but you have langchain-core 0.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-ollama 0.3.2 requires langchain-core<1.0.0,>=0.3.52, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain 0.1.0 requires langchain-core<0.2,>=0.1.7, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain-mistralai 0.2.9 requires langchain-core<1.0.0,>=0.3.47, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain-text-splitters 0.2.2 requires langchain-core<0.3.0,>=0.2.10, but you have langchain-core 0.1.0 which is incompatible.\n",
      "langchain-ibm 0.3.10 requires langchain-core<0.4.0,>=0.3.39, but you have langchain-core 0.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-0.1.0\n",
      "Successfully installed langchain-core-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-ollama in /opt/anaconda3/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-ollama in /opt/anaconda3/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-ollama) (0.4.8)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-ollama) (0.4.8)\n",
      "Collecting langchain-core<1.0.0,>=0.3.52 (from langchain-ollama)\n",
      "  Using cached langchain_core-0.3.54-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.52 (from langchain-ollama)\n",
      "  Using cached langchain_core-0.3.54-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-core<1.0.0,>=0.3.52->langchain-ollama)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain-core<1.0.0,>=0.3.52->langchain-ollama)\n",
      "  Downloading langsmith-0.3.32-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (6.0.1)\n",
      "  Downloading langsmith-0.3.32-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.10.6)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /opt/anaconda3/lib/python3.12/site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.32.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.2.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.10.6)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /opt/anaconda3/lib/python3.12/site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.32.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.0)\n",
      "Using cached langchain_core-0.3.54-py3-none-any.whl (433 kB)\n",
      "Downloading langsmith-0.3.32-py3-none-any.whl (358 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/358.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mRequirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.0)\n",
      "Using cached langchain_core-0.3.54-py3-none-any.whl (433 kB)\n",
      "Downloading langsmith-0.3.32-py3-none-any.whl (358 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.2/358.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.2/358.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langsmith, langchain-core\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.87\n",
      "Installing collected packages: langsmith, langchain-core\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.87\n",
      "    Uninstalling langsmith-0.0.87:\n",
      "      Successfully uninstalled langsmith-0.0.87\n",
      "    Uninstalling langsmith-0.0.87:\n",
      "      Successfully uninstalled langsmith-0.0.87\n",
      "  Attempting uninstall: langchain-core\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.0\n",
      "    Uninstalling langchain-core-0.1.0:\n",
      "      Successfully uninstalled langchain-core-0.1.0\n",
      "    Found existing installation: langchain-core 0.1.0\n",
      "    Uninstalling langchain-core-0.1.0:\n",
      "      Successfully uninstalled langchain-core-0.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.0 requires langchain-core<0.2,>=0.1.7, but you have langchain-core 0.3.54 which is incompatible.\n",
      "langchain 0.1.0 requires langsmith<0.1.0,>=0.0.77, but you have langsmith 0.3.32 which is incompatible.\n",
      "langchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 0.3.54 which is incompatible.\n",
      "langchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.3.32 which is incompatible.\n",
      "langchain-text-splitters 0.2.2 requires langchain-core<0.3.0,>=0.2.10, but you have langchain-core 0.3.54 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-0.3.54 langsmith-0.3.32\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.1.0 requires langchain-core<0.2,>=0.1.7, but you have langchain-core 0.3.54 which is incompatible.\n",
      "langchain 0.1.0 requires langsmith<0.1.0,>=0.0.77, but you have langsmith 0.3.32 which is incompatible.\n",
      "langchain-community 0.0.20 requires langchain-core<0.2,>=0.1.21, but you have langchain-core 0.3.54 which is incompatible.\n",
      "langchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.3.32 which is incompatible.\n",
      "langchain-text-splitters 0.2.2 requires langchain-core<0.3.0,>=0.2.10, but you have langchain-core 0.3.54 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-0.3.54 langsmith-0.3.32\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installations\n",
    "%pip install langchain==0.1.0 | tail -n 1\n",
    "%pip install langchain-ibm==0.0.8 | tail -n 1\n",
    "%pip install langchain-community==0.1.0 | tail -n 1\n",
    "%pip install langchain-core==0.1.0 | tail -n 1\n",
    "%pip install ibm-watsonx-ai | tail -n 1\n",
    "%pip install chromadb | tail -n 1\n",
    "%pip install tiktoken | tail -n 1\n",
    "%pip install bs4 | tail -n 1\n",
    "%pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a889bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/langchain_community/document_loaders/blob_loaders/file_system.py:6: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.blob_loaders.schema import Blob, BlobLoader\n",
      "/opt/anaconda3/lib/python3.12/site-packages/langchain_community/document_loaders/__init__.py:227: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.youtube import (\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/validators.py:767: UserWarning: Mixing V1 and V2 models is not supported. `PromptTemplate` is a V2 model.\n",
      "  warn(f'Mixing V1 and V2 models is not supported. `{type_.__name__}` is a V2 model.', UserWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/langchain_community/document_loaders/__init__.py:227: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.document_loaders.youtube import (\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/validators.py:767: UserWarning: Mixing V1 and V2 models is not supported. `PromptTemplate` is a V2 model.\n",
      "  warn(f'Mixing V1 and V2 models is not supported. `{type_.__name__}` is a V2 model.', UserWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "no validator found for <class 'langchain_core.prompts.prompt.PromptTemplate'>, see `arbitrary_types_allowed` in Config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate, MessagesPlaceholder\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m render_text_description_and_args\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain/prompts/__init__.py:53\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample_selectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     LengthBasedExampleSelector,\n\u001b[1;32m     32\u001b[0m     MaxMarginalRelevanceExampleSelector,\n\u001b[1;32m     33\u001b[0m     SemanticSimilarityExampleSelector,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     36\u001b[0m     AIMessagePromptTemplate,\n\u001b[1;32m     37\u001b[0m     BaseChatPromptTemplate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     load_prompt,\n\u001b[1;32m     51\u001b[0m )\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample_selector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NGramOverlapExampleSelector\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prompt\n\u001b[1;32m     56\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessagePromptTemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseChatPromptTemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain/prompts/example_selector/__init__.py:10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample_selectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlength_based\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     LengthBasedExampleSelector,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample_selectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msemantic_similarity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     MaxMarginalRelevanceExampleSelector,\n\u001b[1;32m      7\u001b[0m     SemanticSimilarityExampleSelector,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample_selector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mngram_overlap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     NGramOverlapExampleSelector,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengthBasedExampleSelector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaxMarginalRelevanceExampleSelector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNGramOverlapExampleSelector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSemanticSimilarityExampleSelector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain/prompts/example_selector/ngram_overlap.py:40\u001b[0m\n\u001b[1;32m     28\u001b[0m     references \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m example]\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\n\u001b[1;32m     31\u001b[0m         sentence_bleu(\n\u001b[1;32m     32\u001b[0m             references,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m         )\n\u001b[1;32m     37\u001b[0m     )\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNGramOverlapExampleSelector\u001b[39;00m(BaseExampleSelector, BaseModel):\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Select and order examples based on ngram overlap score (sentence_bleu score).\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    https://www.nltk.org/_modules/nltk/translate/bleu_score.html\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    https://aclanthology.org/P02-1040.pdf\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     examples: List[\u001b[38;5;28mdict\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/main.py:197\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    190\u001b[0m         is_untouched(value)\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m ann_type \u001b[38;5;241m!=\u001b[39m PyObject\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     ):\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     fields[ann_name] \u001b[38;5;241m=\u001b[39m ModelField\u001b[38;5;241m.\u001b[39minfer(\n\u001b[1;32m    198\u001b[0m         name\u001b[38;5;241m=\u001b[39mann_name,\n\u001b[1;32m    199\u001b[0m         value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    200\u001b[0m         annotation\u001b[38;5;241m=\u001b[39mann_type,\n\u001b[1;32m    201\u001b[0m         class_validators\u001b[38;5;241m=\u001b[39mvg\u001b[38;5;241m.\u001b[39mget_validators(ann_name),\n\u001b[1;32m    202\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ann_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39munderscore_attrs_are_private:\n\u001b[1;32m    205\u001b[0m     private_attributes[ann_name] \u001b[38;5;241m=\u001b[39m PrivateAttr()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/fields.py:504\u001b[0m, in \u001b[0;36mModelField.infer\u001b[0;34m(cls, name, value, annotation, class_validators, config)\u001b[0m\n\u001b[1;32m    501\u001b[0m     required \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    502\u001b[0m annotation \u001b[38;5;241m=\u001b[39m get_annotation_from_field_info(annotation, field_info, name, config\u001b[38;5;241m.\u001b[39mvalidate_assignment)\n\u001b[0;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    505\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    506\u001b[0m     type_\u001b[38;5;241m=\u001b[39mannotation,\n\u001b[1;32m    507\u001b[0m     alias\u001b[38;5;241m=\u001b[39mfield_info\u001b[38;5;241m.\u001b[39malias,\n\u001b[1;32m    508\u001b[0m     class_validators\u001b[38;5;241m=\u001b[39mclass_validators,\n\u001b[1;32m    509\u001b[0m     default\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[1;32m    510\u001b[0m     default_factory\u001b[38;5;241m=\u001b[39mfield_info\u001b[38;5;241m.\u001b[39mdefault_factory,\n\u001b[1;32m    511\u001b[0m     required\u001b[38;5;241m=\u001b[39mrequired,\n\u001b[1;32m    512\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    513\u001b[0m     field_info\u001b[38;5;241m=\u001b[39mfield_info,\n\u001b[1;32m    514\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/fields.py:434\u001b[0m, in \u001b[0;36mModelField.__init__\u001b[0;34m(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m SHAPE_SINGLETON\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mprepare_field(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/fields.py:555\u001b[0m, in \u001b[0;36mModelField.prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;129;01mis\u001b[39;00m Undefined \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulate_validators()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/fields.py:829\u001b[0m, in \u001b[0;36mModelField.populate_validators\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msub_fields \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m SHAPE_GENERIC:\n\u001b[1;32m    826\u001b[0m     get_validators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__get_validators__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    827\u001b[0m     v_funcs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;241m*\u001b[39m[v\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m class_validators_ \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39meach_item \u001b[38;5;129;01mand\u001b[39;00m v\u001b[38;5;241m.\u001b[39mpre],\n\u001b[0;32m--> 829\u001b[0m         \u001b[38;5;241m*\u001b[39m(get_validators() \u001b[38;5;28;01mif\u001b[39;00m get_validators \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(find_validators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config))),\n\u001b[1;32m    830\u001b[0m         \u001b[38;5;241m*\u001b[39m[v\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m class_validators_ \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39meach_item \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mpre],\n\u001b[1;32m    831\u001b[0m     )\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidators \u001b[38;5;241m=\u001b[39m prep_validators(v_funcs)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_validators \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/v1/validators.py:768\u001b[0m, in \u001b[0;36mfind_validators\u001b[0;34m(type_, config)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(type_, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__pydantic_core_schema__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    767\u001b[0m     warn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMixing V1 and V2 models is not supported. `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is a V2 model.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[0;32m--> 768\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno validator found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, see `arbitrary_types_allowed` in Config\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: no validator found for <class 'langchain_core.prompts.prompt.PromptTemplate'>, see `arbitrary_types_allowed` in Config"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import getpass\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa1014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model = \"stablelm2\",\n",
    "    temperature = 0.8,\n",
    "    # num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2b51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"Answer the {query} accurately. If you do not know the answer, simply say you do not know.\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1532412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60abf6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The sport that is predominantly played at the U.S. Open, a major professional tennis tournament, is tennis. The U.S. Open, first held in 1877 and now called the US Open Series, is one of the four Grand Slam tournaments in tennis.\\n\\nTennis, as we know it today, was not invented until the late 19th century by professionals and amateur players alike. However, the sport did not become popular enough to be considered a major international sporting event until the mid-20th century. As a result, competitions were initially limited to club and amateur levels.\\n\\nThe U.S Open, as well as other important tournaments such as Wimbledon and the Australian Open, serves as a platform for professional tennis players to showcase their skills on an international level. The tournament has grown in popularity over the years due to its prestigious status among tennis enthusiasts worldwide.\\n\\nIn addition to tennis, there are also events within the U.S. Open Series that involve other sports, such as golf and soccer, but the primary sport played during the event is tennis.', additional_kwargs={}, response_metadata={'model': 'stablelm2', 'created_at': '2025-04-19T20:03:50.967876Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9648685833, 'load_duration': 36936125, 'prompt_eval_count': 48, 'prompt_eval_duration': 2551670708, 'eval_count': 216, 'eval_duration': 7044031625, 'model_name': 'stablelm2'}, id='run-d2e725e4-9b25-451c-8be2-e9bc06c1a055-0', usage_metadata={'input_tokens': 48, 'output_tokens': 216, 'total_tokens': 264})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent.invoke({\"query\": \"What sport is played at the US Open?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629dc290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The U.S. Open Tennis Championships, also known as the Grand Slam, is one of the four major tennis championships held annually on the United States mainland. The tournament takes place at different venues throughout the years due to its location changeover.\\n\\nIn 2021, the U.S. Open took place in New York City and Queens from September 14th to October 4th, at the USTA Billie Jean King Center (formerly Madison Square Garden) and the Grand Hyatt New York, respectively.\\n\\nHowever, it\\'s essential to note that since 2016, due to financial and logistical reasons, the tournament has been held in Flushing Meadows-Cornerstone Park, near the site of the former U.S. Open venue at the World Court. In this case, the event was named the \"US Open\" for short periods during its transition.\\n\\nSo, to summarize: The 2021 U.S. Open Tennis Championship took place in New York City and Queens (USTA Billie Jean King Center and Grand Hyatt New York). For events that occurred before 2021 (like 2019 and earlier), the location was Flushing Meadows-Cornerstone Park, near the former site of the U.S. Open at the World Court.', additional_kwargs={}, response_metadata={'model': 'stablelm2', 'created_at': '2025-04-19T20:04:03.12975Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7082835500, 'load_duration': 32099458, 'prompt_eval_count': 50, 'prompt_eval_duration': 502179959, 'eval_count': 266, 'eval_duration': 6531044375, 'model_name': 'stablelm2'}, id='run-9f4ccd9c-4bd2-4255-8d5b-58ace2372488-0', usage_metadata={'input_tokens': 50, 'output_tokens': 266, 'total_tokens': 316})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"query\": \"Where was the 2024 US Open Tennis Championship?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed04fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.ibm.com/case-studies/us-open\",\n",
    "    \"https://www.ibm.com/sports/usopen\",\n",
    "    \"https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement\",\n",
    "    \"https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87cb6293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.ibm.com/case-studies/us-open', 'title': 'U.S. Open | IBM', 'description': 'To help the US Open stay on the cutting edge of customer experience, IBM Consulting built powerful generative AI models with watsonx.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\nU.S. Open | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nUS Open \\n\\n\\n\\n\\n\\n\\n                \\n\\n\\n\\n  \\n    Acing the US Open digital experience\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n            \\n\\n                    \\n\\n\\n  \\n  \\n      AI models built with watsonx transform data into insight\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                \\n\\n\\nGet the latest AI and tech insights\\n\\n\\n\\nLearn More\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFor two weeks at the end of summer, nearly one million people make the journey to Flushing, New York, to watch the best tennis players in the world compete in the US Open Tennis Championships.\\nYear after year, it is one of the most highly attended sporting events in the world.\\n\\nBut more than 15 million global tennis fans follow the tournament through the US Open app and website. And to keep them coming back for more, the United States Tennis Association (USTA) has worked side-by-side with IBM Consulting® for more than three decades, developing and delivering a world-class digital experience that constantly advances its features and functionality.\\n\\n“The digital experience of the US Open is of enormous importance to our global fans, and therefore to us,” says Kirsten Corio, Chief Commercial Officer at the USTA. “That means we need to constantly innovate to meet the modern demands of tennis fans, anticipating their needs, but also surprising them with new and unexpected experiences.”\\nTo help the US Open stay on the cutting edge of customer experience, IBM Consulting worked closely with the USTA to develop generative AI models that transform tennis data into insights and original content on the US Open app and website. To do this, the USTA used IBM® watsonx™, a portfolio of AI products, and powerful AI models, including IBM Granite™ foundation models, to help develop key app features, such as Match Reports and AI Commentary for US Open highlight reels.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n      15M\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\nWorld-class digital experiences for more than 15 million fans around the globe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n      7M\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\nIBM captures and analyzes more than 7 million data points throughout the tournament\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n            \\n                \\n                     \\n                        \\n\\n\\n  \\n  \\n      The AI models built with watsonx do more than enhance the digital experience of the US Open. They also scale the productivity of our editorial team by automating key workflows.\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                        \\n                \\n            \\n        \\n        \\n            Kirsten Corio\\n        \\n\\n            Chief Commercial Officer\\n        \\n\\n            United States Tennis Association\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    Generative AI experiences, built with watsonx\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nThe US Open is a sprawling, two-week tournament, with hundreds of matches played on 22 different courts. Keeping up with all the action is a challenge, both for tennis fans and the USTA editorial team covering the event. So, the USTA asked IBM to design, develop, and deliver solutions that enhance the digital experience and help its team serve up more content, covering more matches throughout the tournament.\\nTo do it, the IBM Consulting team built generative AI-powered features using the watsonx, a part of IBM\\'s portfolio of AI products. For example, Match Reports are AI-generated post-match summaries that are designed to get fans quickly up to speed on the action from around the tournament. AI Commentary adds AI-generated, spoken commentary to match highlights. And SlamTracker–the premier scoring application for the US Open–features AI-generated match previews and recaps.\\n“The AI models built with watsonx do more than enhance the digital experience of the US Open,” says Kirsten Corio, Chief Commercial Officer, USTA. “They also scale the productivity of our editorial team by automating key workflows.”\\nThe IBM team worked with multiple AI models to develop the new features, including the family of Granit AI models. These large language models already understand language, but they needed to be trained, or “tuned,” on tennis data in order to translate US Open action into sentences and summaries.\\n“Foundation models are incredibly powerful and are ushering in a new age of generative AI,” says Shannon Miller, a Partner at IBM Consulting. “But to generate meaningful business outcomes, they need to be trained on high-quality data and develop domain expertise. And that’s why an organization’s proprietary data is the key differentiator when it comes to AI.”\\nThe team used watsonx.data to connect and curate the USTA’s trusted data sources. The curation process includes de-duping and filtering the foundational data that informs the large language model, as well as the USTA’s proprietary data. The process filters for things like profanity or abusive language and objectionable content.\\nThe models were then trained to translate tennis data into cogent descriptions, summarizing entire matches in the case of Match Reports, or generating sentences that describe the action in highlight reels for AI Commentary. Over the course of the 2024 US Open, Match Reports and AI Commentary will be generated for all men’s and women’s singles matches; something the USTA editorial team has never done before. And the ongoing operation of the models is monitored and managed using elements of watsonx.governance, which ensures the AI is performant, compliant and operating as expected.\\nDuring the software development phase of the project, the team took advantage of a powerful generative AI assistant to increase the efficiency and accuracy of its code. IBM watsonx Code Assistant™ uses generative AI from a purpose-built Granite model to accelerate software development, helping developers generate code based on natural language prompts. The team used this tool to analyze and explain snippets of code, annotate code to facilitate better collaboration between developers, and auto-complete snippets of analyzed code.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    Platform of innovation\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nTo develop new capabilities every year, the USTA needs to move with speed and purpose. The process starts the week after the US Open concludes, when IBM Consulting kicks off work using the IBM Garage™ Methodology, a highly collaborative approach to co-creation.\\n“When we engage with a client, it’s critical that we work closely together every step of the way, ideating, iterating and adapting as we drive toward the client’s desired end state,” says Miller.\\nIn order to transform new ideas into digital reality, IBM Consulting designs, develops, and manages a powerful digital infrastructure capable of processing structured and unstructured data, and integrating technology from a variety of sources. This foundational infrastructure is advanced and improved upon every year.\\n“It used to be that innovation cycles were measured in years,” says the USTA’s Corio. “But now, innovation is measured in weeks and days, and it can come from anywhere. So, we needed a flexible platform that could handle all kinds of data, automate the process of turning data into insight, and do it all while protecting the entire digital environment.”\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    From data to insight\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nThe raw material of any digital experience is data, and the US Open tournament produces a lot of it. For starters, each US Open consists of 128 men and 128 women singles players, and a total of seven rounds for each tournament. Each tennis player comes with his or her own data set, including world ranking and recent performance. But that’s just the beginning.\\nOver the course of the tournament, more than 125,000 points will be played. And each one of those points generates its own data set: serve direction, speed, return shot type, winner shot type, rally count and even ball position. All told, more than seven million data points are generated during the tournament.\\nTo add more texture and context to the US Open digital experience, the team wanted to go beyond the numbers. So, they used AI to analyze the language and sentiment of millions of articles from hundreds of thousands of different sources to develop insights that are unique and informative, for instance, the likelihood to Win predictions for all singles matches. To help manage the collection, integration, and analysis of the data sets, IBM used IBM watsonx.data™, a purpose-built data store specifically designed to handle AI workloads.\\n“It’s a massive data management operation, incorporating multiple sources of data and a variety of partners,” says Miller. “But the magic happens when you combine hard data like stats and scores with unstructured data like media commentary. That is what gives tennis fans a more complete picture of each match.”\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    Automation, containerization and other efficiencies\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nTo streamline this process, during the years working with the UTSA, IBM Consulting built automated workflows that integrate and orchestrate the flow of data through the various applications and AI models needed to produce the digital experience. These workflows are made possible by a hybrid cloud architecture and the containerized apps running on Red Hat®\\xa0OpenShift®\\xa0on IBM Cloud.\\xa0The US Open hybrid multicloud architecture is made up of four public clouds, drawing on data from a variety of sources and integrating features and capability from a variety of partners. By containerizing the applications, the team can write them once and run them anywhere, ensuring that the right data gets to the right application on the right cloud.\\n\\n“With this platform, we’re capable of doing things that were not possible just a few years ago,” says Corio. “Managing all that data, producing AI-generated insights, securing the environment … IBM just makes it all come together for us. And I can’t wait to see what the future of the partnership holds.”\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n            \\n                \\n                     \\n                        \\n\\n\\n  \\n  \\n      Managing all that data, producing AI-generated insights, securing the environment…IBM just makes it all come together for us. And I can’t wait to see what the future of the partnership holds.\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                        \\n                \\n            \\n        \\n        \\n            Kirsten Corio\\n        \\n\\n            Chief Commercial Officer\\n        \\n\\n            United States Tennis Association\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    About United States Tennis Association (USTA)\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nFounded in 1881, the USTAis the national governing body for the sport of tennis in the US. The US Open s the association’s Grand Slam tournament, first held in 1968—the year that Arthur Ashe won the men’s singles title. The US Open is played each September at the USTA Billie Jean King National Tennis Center in Flushing, Queens, New York.\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n  \\n  \\n      Solution components\\n  \\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n        IBM Consulting™\\n        \\n        \\n    \\n\\n        IBM Garage™ Methodology\\n        \\n        \\n    \\n\\n        IBM® Granite™\\n        \\n        \\n    \\n\\n        IBM watsonx™\\n        \\n        \\n    \\n\\n        IBM watsonx.data™\\n        \\n        \\n    \\n\\n        IBM watsonx Code Assistant™\\n        \\n        \\n    \\n\\n        Red Hat® OpenShift® on IBM Cloud\\n        \\n        \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIBM watsonx\\n\\n\\n\\n\\nWant AI to make smart use of all your data? Use IBM watsonx to accelerate the fine tuning and deployment of your models.\\n\\n\\n\\n\\n\\nLearn more about watsonx\\n\\n\\nView more case stories\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    The Masters\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nWhat if the Masters could turn data into insight?\\n\\n\\n\\nRead the case study\\n            \\n        \\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Wimbledon\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nIBM and Wimbledon, a partnership of innovation\\n\\n\\n\\nRead the case study\\n            \\n        \\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Wintershall Dea AG\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nDrilling down into data to transform the oil and gas industry\\n\\n\\n\\nRead the case study\\n            \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            \\n\\n\\n\\n  \\n    Legal\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                        \\n\\n\\n\\n\\n\\n© Copyright IBM Corporation 2023. IBM Corporation, IBM Consulting, New Orchard Road, Armonk, NY 10504\\nProduced in the United States of America, August 2024.\\nIBM, the IBM logo, ibm.com, IBM Consulting, IBM Garage, Granite, watsonx, watsonx.data, and Code Assistant are trademarks or registered trademarks of International Business Machines Corporation, in the United States and/or other countries. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on ibm.com/trademark.\\n\\nRed Hat® and OpenShift® are registered trademarks of Red Hat, Inc. or its subsidiaries in the United States and other countries.\\xa0\\n\\nThis document is current as of the initial date of publication and may be changed by IBM at any time. Not all offerings are available in every country in which IBM operates.\\nAll client examples cited or described are presented as illustrations of the manner in which some clients have used IBM products and the results they may have achieved. Actual environmental costs and performance characteristics will vary depending on individual client configurations and conditions. Generally expected results cannot be provided as each client\\'s results will depend entirely on the client\\'s systems and services ordered. THE INFORMATION IN THIS DOCUMENT IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY, EXPRESS OR IMPLIED, INCLUDING WITHOUT ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OR CONDITION OF NON-INFRINGEMENT. IBM products are warranted according to the terms and conditions of the agreements under which they are provided.\\nStatement of Good Security Practices: No IT system or product should be considered completely secure, and no single product, service or security measure can be completely effective in preventing improper use or access.\\xa0 IBM does not warrant that any systems, products or services are immune from, or will make your enterprise immune from, the malicious or illegal conduct of any party.\\nThe client is responsible for ensuring compliance with all applicable laws and regulations. IBM does not provide legal advice nor represent or warrant that its services or products will ensure that the client is compliant with any law or regulation.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "docs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "351d0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n",
    "    url=credentials[\"url\"],\n",
    "    apikey=credentials[\"apikey\"],\n",
    "    project_id=project_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192da1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d58466",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Chroma.__init__() got an unexpected keyword argument 'embedding_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m      2\u001b[0m     documents\u001b[38;5;241m=\u001b[39mdoc_splits,\n\u001b[1;32m      3\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magentic-rag-chroma\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     embedding_functions\u001b[38;5;241m=\u001b[39mdefault_ef,\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:778\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    777\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m    779\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    780\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    781\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    782\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    783\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    784\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m    785\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m    786\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    787\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    789\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:714\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    694\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    695\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    715\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    716\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    717\u001b[0m         persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m    718\u001b[0m         client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m    719\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    720\u001b[0m         collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    722\u001b[0m     )\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid1()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "\u001b[0;31mTypeError\u001b[0m: Chroma.__init__() got an unexpected keyword argument 'embedding_functions'"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"agentic-rag-chroma\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c66df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
